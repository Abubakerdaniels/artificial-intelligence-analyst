{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Image recognition with TensorFlow and Keras\n\nIn this notebook we will use computer vision, TensorFlow, and Keras for image classification and processing.\nThis tutorial is originated based on [Image recognition with TensorFlow and Keras tutorial](https://developer.ibm.com/technologies/vision/articles/image-recognition-challenge-with-tensorflow-and-keras-pt1/)\nUsing images, we will be able to classify whether a given image is a chihuahua (a dog breed) or a muffin.\n\nThe data set used with this article is formed by combining this [source](https://github.com/ieee8023/deep-learning-datasets/tree/master/chihuahua-muffin)and searching the internet and applying some basic image processing techniques. The images in this data set are collected, used, and provided under the Creative commons fair usage policy. The intended use is (for scientific research in image recognition using artificial neural networks) by using the TensorFlow and Keras library. This solution applies the same techniques as given in https://www.tensorflow.org/tutorials/keras/basic_classification\n\n## Prerequisites\n\nTo complete the tutorial, you need an [IBM Cloud account](https://cloud.ibm.com/registration?cm_sp=ibmdev-_-developer-tutorials-_-cloudreg). You can obtain a free trial account, which gives you access to IBM Cloud and [IBM Watson Studio](https://www.ibm.com/cloud/watson-studio).\nYou need to create a Watson Studio project and upload the images data into your assets. \n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## Import TensorFlow, Keras, and other helper libraries\n\nStart by installing the required libraries using pip. Then followed by importing these libraries. \nYou will use TensorFlow and Keras for running the machine learning and the Pillow Python library for image processing."}, {"metadata": {}, "cell_type": "code", "source": "!pip install tensorflow matplotlib pillow", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: tensorflow in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: matplotlib in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (3.2.2)\nRequirement already satisfied: pillow in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (7.2.0)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: astor>=0.6.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.8.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: gast==0.2.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.2.2)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.27.2)\nRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (3.12.3)\nRequirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.34.2)\nRequirement already satisfied: keras-applications>=1.0.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.0.8)\nCollecting scipy==1.4.1; python_version >= \"3\"\n  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 8.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.9.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.18.5)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib) (2.8.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib) (0.10.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.22.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.24.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (47.3.1.post20200622)\nRequirement already satisfied: h5py in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\nRequirement already satisfied: aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.6.2)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.1.1)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2020.12.5)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (19.3.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.5.1)\nRequirement already satisfied: multidict<5.0,>=4.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.7.6)\nRequirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\nRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.7.4.2)\nInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.5.0\n    Uninstalling scipy-1.5.0:\n      Successfully uninstalled scipy-1.5.0\nSuccessfully installed scipy-1.4.1\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Importing the Python libraries."}, {"metadata": {}, "cell_type": "code", "source": "import tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob, os\nimport re\n\n# Pillow\nimport PIL\nfrom PIL import Image", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Import the data\n\nA Python function to preprocess input images and convert them to grey scale.\n\nUsing colored image (like in RGB for instance) means that we have 3 dimensions, while grayscale has just one dimension. To avoid unnecessary cost in computation and since color has no significance in these images we will go ahead and convert images to gray scale.\n\n"}, {"metadata": {}, "cell_type": "code", "source": "# Use Pillow library to convert an input jpeg to a 8 bit grey scale image array for processing.\ndef jpeg_to_8_bit_greyscale(path, maxsize):\n        img = Image.open(path).convert('L')   # convert image to 8-bit grayscale\n        # Make aspect ratio as 1:1, by applying image crop.\n    # Please note, croping works for this data set, but in general one\n    # needs to locate the subject and then crop or scale accordingly.\n        WIDTH, HEIGHT = img.size\n        if WIDTH != HEIGHT:\n                m_min_d = min(WIDTH, HEIGHT)\n                img = img.crop((0, 0, m_min_d, m_min_d))\n        # Scale the image to the requested maxsize by Anti-alias sampling.\n        img.thumbnail(maxsize, PIL.Image.ANTIALIAS)\n        return np.asarray(img)\n", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "A Python function to load the data set from images, into numpy arrays:"}, {"metadata": {}, "cell_type": "code", "source": "def load_image_dataset(path_dir, maxsize):\n        images = []\n        labels = []\n        os.chdir(path_dir)\n        for file in glob.glob(\"*.jpg\"):\n                img = jpeg_to_8_bit_greyscale(file, maxsize)\n                if re.match('chihuahua.*', file):\n                        images.append(img)\n                        labels.append(0)\n                elif re.match('muffin.*', file):\n                        images.append(img)\n                        labels.append(1)\n        os.chdir(\"../\")\n        return (np.asarray(images), np.asarray(labels))\n", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We should scale the images to some standard size smaller than actual image resolution. These images are more than 170\u00d7170, so we scale them all to 100\u00d7100 for further processing:"}, {"metadata": {}, "cell_type": "code", "source": "maxsize = 100, 100", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "From watson Studio layout, you will need to import the data. First make sure that your data is imported in your watson Studio project under data assets tab. Then select the following cell,open the data panel from 0100 icon in the upper right corner and choose your data. From Insert to code, select \"Streamingbody object\""}, {"metadata": {}, "cell_type": "code", "source": "#From Data panel, select chihuahua-muffin_trainingset and select  insert to code Streamingbody object ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_6a41ba9f24ef477288c56272e3442852 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='lM733kCKvPqhXoAGliVRkntr-QUMPo0Gl1fa90YY8AFL',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\n# Your data file was loaded into a botocore.response.StreamingBody object.\n# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n# pandas documentation: http://pandas.pydata.org/\nstreaming_body_1 = client_6a41ba9f24ef477288c56272e3442852.get_object(Bucket='watsonstudiooverview-donotdelete-pr-fzhhnz3ifzwjkr', Key='chihuahua-muffin_trainingset.zip')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(streaming_body_1, \"__iter__\"): streaming_body_1.__iter__ = types.MethodType( __iter__, streaming_body_1 ) ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now you will need to unzip your training data.\nNote: make sure that the variable name for streaming body (generated from previous cell) is the one used for unzipping. for example it is streaming_body_1"}, {"metadata": {}, "cell_type": "code", "source": "from io import BytesIO\nimport zipfile\n\nzip_ref = zipfile.ZipFile(BytesIO(streaming_body_1.read()), 'r')\nzip_ref.extractall()\nzip_ref.close()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#From Data panel, select chihuahua-muffin_testset and select  insert to code Streamingbody object ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You will also need to unzip your test data.\nNote: make sure that the variable name for streaming body (generated from previous cell) is the one used for unzipping. for example it is streaming_body_2"}, {"metadata": {}, "cell_type": "code", "source": "zip_ref2 = zipfile.ZipFile(BytesIO(streaming_body_2.read()), 'r')\nzip_ref2.extractall()\nzip_ref2.close()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Load the data\n\nlet uss execute the following functions and load training and test data sets:\ntrain_images and train_lables is training data set.\ntest_images and test_labels is testing data set for validating the model\u2019s performance against unseen data.\n"}, {"metadata": {}, "cell_type": "code", "source": "(train_images, train_labels) = load_image_dataset('chihuahua-muffin_trainingset', maxsize)\n(test_images, test_labels) = load_image_dataset('chihuahua-muffin_testset', maxsize)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, we define the class names for our data set. Because this data has only two classes (an image can either be a Chihuahua or a Muffin), we have class_names as follows:"}, {"metadata": {}, "cell_type": "code", "source": "class_names = ['chihuahua', 'muffin']\ntrain_images.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Explore the data\n\nIn this data set, we have 26 training examples, of both Chihuahua and muffin images:"}, {"metadata": {}, "cell_type": "markdown", "source": "Each image has its respective label \u2013 either a 0 or 1. A 0 indicates a class_names[0] i.e. a chihuahua and 1 indicates class_names[1] i.e. a muffin:"}, {"metadata": {}, "cell_type": "code", "source": "print(train_labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "For test set, we have 14 examples, seven for each class:"}, {"metadata": {}, "cell_type": "code", "source": "test_images.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(test_labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Visualize the data set\n\nUsing the matplotlib.pyplot Python library, we can visualize our data. Make sure you have the matplotlib library installed.\n\nFollowing Python helper function helps us draw these images on our screen:"}, {"metadata": {}, "cell_type": "code", "source": "def display_images(images, labels):\n        plt.figure(figsize=(10,10))\n        grid_size = min(25, len(images))\n        for i in range(grid_size):\n                plt.subplot(5, 5, i+1)\n                plt.xticks([])\n                plt.yticks([])\n                plt.grid(False)\n                plt.imshow(images[i], cmap=plt.cm.binary)\n                plt.xlabel(class_names[labels[i]])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let\u2019s visualize the training data set, as follows:\nNote: The images are grayscaled and cropped in the preprocessing step of our images at the time of loading."}, {"metadata": {}, "cell_type": "code", "source": "display_images(train_images, train_labels)\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Preprocess the data\nScaling the images to values between 0 and 1"}, {"metadata": {}, "cell_type": "code", "source": "train_images = train_images / 255.0\ntest_images = test_images / 255.0", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Build the model\n\nSet up the layers, We have used four layers total. The first layer is to simply flatten the data set into a single array and does not get training. The other three layers are dense and use sigmoid as activation function:"}, {"metadata": {}, "cell_type": "code", "source": "# Setting up the layers.\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(100, 100)),\n        keras.layers.Dense(128, activation=tf.nn.sigmoid),\n        keras.layers.Dense(16, activation=tf.nn.sigmoid),\n    keras.layers.Dense(2, activation=tf.nn.softmax)\n])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Compile the model\n\nThe optimizer is stochastic gradient descent (SGD):"}, {"metadata": {}, "cell_type": "code", "source": "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-5, momentum=0.7, nesterov=True)\n\nmodel.compile(optimizer=sgd,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Train the model:\nThis may take some time, observe the changes in loss and accuracy."}, {"metadata": {}, "cell_type": "code", "source": "model.fit(train_images, train_labels, epochs=100)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Evaluate accuracy\nYou may notice that test accuracy is less than training accuracy. This indicates model has overfit the data. \nWith recent advances in image recognition and using more training data, we can perform much better on this data set challenge."}, {"metadata": {}, "cell_type": "code", "source": "test_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('Test accuracy:', test_acc)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Make predictions\n\nTo make predictions, we can simply call predict on the generated model:"}, {"metadata": {}, "cell_type": "code", "source": "predictions = model.predict(test_images)\nprint(predictions)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Finally, display images and see how the model performed on test set:"}, {"metadata": {}, "cell_type": "code", "source": "display_images(test_images, np.argmax(predictions, axis = 1))\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Conclusion\n\nAt this point you have learned how to train a neural network using Tensorflow and Keras. As you see there are a few wrong classifications in our result. Do you have ideas for how to enhance the model? If you want to learn more feel free to check methods of model enhancing in [part2](https://developer.ibm.com/articles/image-recognition-challenge-with-tensorflow-and-keras-pt2/) of this tutorial."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}
